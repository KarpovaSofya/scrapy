{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/Scrapy_logo.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy  является основой приложения для обхода веб-сайтов и извлечения структурированных данных, которые могут быть использованы для широкого спектра полезных приложений, таких как интеллектуальный анализ данных, обработки информации или архивирования истории.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy – одна из наиболее популярных и производительных библиотек Python для получения данных с веб-страниц, которая включает в себя большинство общих функциональных возможностей. Это значит, что вам не придётся самостоятельно прописывать многие функции. Scrapy позволяет быстро и без труда создать «веб-паука»."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Официальная документация: https://docs.scrapy.org/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Установка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С помощью pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/cmd2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако, установить библиотеку через pip никик не получалось, поэтому проишлось устанавливать Anacondas и работать через неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание проекта"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Официальная документация: https://docs.scrapy.org/en/latest/intro/tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "До того, как начать scraping, вам необходимо создать новый Scrapy прект (Scrapy project). Перейдите в директорию, где вы хотите разместить ваш проект и выполните:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получится:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/cmd3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например middlewares.py:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание паука"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пауки - это классы, которые определяет пользователь, и которые Scrapy использует для сбора данных (scraping) с веб-сайта (или группы веб-сайтов). Они должны ,быть подклассом  scrapy.Spider и определять первоначальные запросы и методы, чтобы переходить по ссылкам на страницах и  анализировать  содержимое страницы для извлечения данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим паука first_spider в папке /spiders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"first_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'http://quotes.toscrape.com/page/1/',\n",
    "            'http://quotes.toscrape.com/page/2/',\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)\n",
    "        self.log('Saved file %s' % filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как можно заметить сощданный паук ябляется подклассом scrapy.Spider и определяет некоторые атрибуты и методы:\n",
    "- name: определяет паука. Оно должно быть уникальным для каждого паука.\n",
    "- start_requests(): метод должен возвращать итеририруемые запросы (можно возвращать список запросов), по которые паук и начнёт проходить. Последующие запросы будут создаваться последовательно на основе этих первоначальных запросов.\n",
    "- parse(): метод, который будет вызван для обработки ответа, загруженного для каждого из сделанных запросов. Параметр response является экземпляром TextResponse, который содержит содержимое страницы и имеет дополнительные полезные методы для его обработки. parse() - метод, который просматривает ответ, поочерёдно извлекая необходимые данные, а также находит новые URL-адреса для продолжния и создаёт новые запросы для них.\n",
    "\n",
    "В данном примере паук first_spider собирает данные с http://quotes.toscrape.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный код можно немного сократить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"first_spider\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = 'quotes-%s.html' % page\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы отдельно не определяем мотод start_requests(), а передаём необходимые URL-адреса как метод. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запуск паука"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl first_spider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/cmd4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В папке spiders можно увидеть два новых файла quotes1-html и quotes2-html, содержащие соответствующие URL-адреса, как было указано в методе parse()\n",
    "<img src =\"files/images/4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy собирает объекты типа scrapy.Request, возвращённые методом start_requests(). Получив ответ для каждого из них, он создает экземпляры объектов ответа и вызывает метод обратного вызова, связанный с запросом (в данном случае, метод parse), передавая ответ в качестве аргумента."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Извлечение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший способ научиться извлекать данные с помощью Scrapy - это использование селекторов с помощью оболлочки  Scrapy shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy shell \"http://quotes.toscrape.com/page/1/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/cmd5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим response.css(\"title\")\n",
    "<img src =\"files/images/cmd6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'\" - объект SelectorList, который показывает пользователю список селекторов, что позволяет далее исследовать XML/HTML элементы, выполнять более мелкие селекторы и извлекать данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title::text').extract()\n",
    "['Quotes to Scrape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title').extract()\n",
    "[<title>Quotes to Scrape</title>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title::text').extract_first()\n",
    "'Quotes to Scrape'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.css('title::text').extract_first()\n",
    "'Quotes to Scrape'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/cmd7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извлечение цитат и аторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если мы запустим \n",
    "response.css(\"div.quote\"), Scrapy выведет список из десяти объектов исследуемой страницы.\n",
    "<img src =\"files/images/cmd8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Исследуем первый из них. \n",
    "<img src =\"files/images/5.png\">\n",
    "Выполним следущие команды:\n",
    "1. quote = response.css(\"div.quote\")[0]\n",
    "2. title = quote.css(\"span.text::text\").extract_first()\n",
    "3. author = quote.css(\"small.author::text\").extract_first()\n",
    "<img src =\"files/images/cmd9.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перепишем паука:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"first_spider\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "        'http://quotes.toscrape.com/page/2/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/cmd10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018-11-11 20:21:26 [scrapy.core.scraper] DEBUG: Scraped from <200 http://quotes.toscrape.com/page/1/>\n",
    "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Запишем полученные данае в файл quotes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запустим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl first_spider -o quotes.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/6.png\">\n",
    "<img src =\"files/images/7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При необходимости данные можно записывать в файлы с расширениями 'json', 'jsonlines', 'jl', 'csv', 'xml', 'marshal', 'pickle'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтение нескольких страниц"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"first_spider\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com/page/1/',\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом паук first_spider, пройдёт и соберёт данные со всех страниц."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Использование атрибутов паука"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы можете предоставить аргументы командной строки для ваших пауков с помощью -a при их запуске:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl quotes -o quotes-humor.json -a tag=humor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"first_spider\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        url = 'http://quotes.toscrape.com/'\n",
    "        tag = getattr(self, 'tag', None)\n",
    "        if tag is not None:\n",
    "            url = url + 'tag/' + tag\n",
    "        yield scrapy.Request(url, self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').extract_first(),\n",
    "                'author': quote.css('small.author::text').extract_first(),\n",
    "                'tags': quote.css('div.tags a.tag::text').extract(),\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(href)').extract_first()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"files/images/9.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
